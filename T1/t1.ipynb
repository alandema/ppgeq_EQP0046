{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, signal\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('skab.csv', sep=\";\", index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(df.corr(), cmap='seismic')\n",
    "plt.colorbar()\n",
    "plt.gca().set_xticks(np.arange(len(df.columns)))\n",
    "plt.gca().set_yticks(np.arange(len(df.columns)))\n",
    "plt.gca().set_xticklabels(labels=df.columns)\n",
    "plt.gca().set_yticklabels(labels=df.columns)\n",
    "plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=len(df.columns), figsize=(35, 6))\n",
    "\n",
    "for i, col in enumerate(df):\n",
    "    axs[i].plot(df[col])\n",
    "    axs[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers por IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers(column):\n",
    "    column_iqr = stats.iqr(column)\n",
    "    lower_bound = np.percentile(column, 25) - 1.5 * column_iqr\n",
    "    upper_bound = np.percentile(column, 75) + 1.5 * column_iqr\n",
    "    return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "df_iqr = df.apply(filter_outliers, axis=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=len(df.columns), figsize=(35, 6))\n",
    "for i, col in enumerate(df):\n",
    "    axs[i].plot(df[col])\n",
    "    axs[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers por hampel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "peguei de https://github.com/dwervin/pyhampel\n",
    "\n",
    "tem tambem o pacote hampel pra ser instalado via pip, mas precisa instalar o C++ compiler e eu nÃ£o tava afim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hampel_filter_df(df: pd.DataFrame, vals_col: str, time_col=None, win_size=30, num_dev=3, center_win=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes in dataframe containing time series of values, applies Hampel filter on\n",
    "    these values, and returns dataframe consisting of original values columns along with\n",
    "    the Hampel filtered data, outlier values and boolean flags where outliers found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        data from containing time series that needs to be Hampel filtered\n",
    "    vals_col: str\n",
    "        Single column name that contains values that need to be filtered.\n",
    "    time_col: str\n",
    "        Name of column that contains dates or timestamps\n",
    "    win_size: int\n",
    "        Size of sliding window for filtering.  Essentially the number of time steps to be considered when filtering.\n",
    "    num_dev: int\n",
    "        Number of standard deviations to consider when detecting values that would be considered outliers.\n",
    "    center_win: Boolean\n",
    "        Boolean value that determines whether the window is centered about the point being filtered?  Default=True.\n",
    "        If False, point is at the leading edge (i.e. right side) of window  calculation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Function returns a full dataframe consisting of original values columns along with\n",
    "    the Hampel filtered data, outlier values and boolean flags where outliers found.\n",
    "    \"\"\"\n",
    "    # print(\"IN HAMPEL_FILTER_DF\")\n",
    "\n",
    "    if (time_col != None):\n",
    "        if (time_col not in list(df.columns)):\n",
    "            raise Exception(\"Timestamp column '{}' is missing!\".format(time_col))\n",
    "        elif (time_col in list(df.columns)):\n",
    "            if (not np.issubdtype(df[time_col].dtype, np.datetime64)):\n",
    "                if (not np.issubdtype(pd.to_datetime(df[time_col]).dtype, np.datetime64)):\n",
    "                    raise Exception(\"Timestamp column '{}' is not np.datetime64\".format(time_col))\n",
    "                else:\n",
    "                    df[time_col] = pd.to_datetime(df[time_col])\n",
    "                    drop_cols = set(df.columns) - set([time_col, vals_col])\n",
    "                    # Not really filtered at this point. Just naming appropriately ahead of time.\n",
    "                    orig_vals = df.sort_values(time_col, ascending=True).set_index(time_col).copy()\n",
    "                    filtered = orig_vals.drop(columns=drop_cols).copy()\n",
    "            else:\n",
    "                df[time_col] = pd.to_datetime(df[time_col])\n",
    "                drop_cols = set(df.columns) - set([time_col, vals_col])\n",
    "                # Not really filtered at this point. Just naming appropriately ahead of time.\n",
    "                orig_vals = df.sort_values(time_col, ascending=True).set_index(time_col).copy()\n",
    "                filtered = orig_vals.drop(columns=drop_cols).copy()\n",
    "\n",
    "    elif (time_col == None):\n",
    "        if (not isinstance(df.index, pd.DatetimeIndex)):\n",
    "            raise Exception(\"DataFrame index is not pd.DatetimeIndex\")\n",
    "        else:\n",
    "            df.sort_index(inplace=True)\n",
    "            drop_cols = set(df.columns) - set([vals_col])\n",
    "            orig_vals = df.copy()\n",
    "            filtered = orig_vals.drop(columns=drop_cols).copy()\n",
    "\n",
    "    # Scale factor for estimating standard deviation based upon median value\n",
    "    L = 1.4826\n",
    "\n",
    "    # Calculate rolling median for the series\n",
    "    rolling_median = filtered.rolling(window=int(win_size), center=center_win, min_periods=1).median()\n",
    "\n",
    "    # Define a lambda function to apply to the series to calculate Median Absolute Deviation\n",
    "    def MAD(x): return np.median(np.abs(x - np.median(x)))\n",
    "\n",
    "    # Calculate rolling MAD series\n",
    "    rolling_MAD = filtered.rolling(window=(win_size), center=center_win, min_periods=1).apply(MAD)\n",
    "\n",
    "    # Calculate threshold level for filtering based upon the number of standard deviation and\n",
    "    # constant scaling factor L.\n",
    "    threshold = int(num_dev) * L * rolling_MAD\n",
    "\n",
    "    # Difference between original values and rolling median\n",
    "    # Again, \"filtered\" not yet filtered at this point.\n",
    "    difference = np.abs(filtered - rolling_median)\n",
    "\n",
    "    '''\n",
    "    # TODO: Look at logic here to possibly not mark as an outlier if threshold value\n",
    "    is 0.0\n",
    "    '''\n",
    "\n",
    "    # Flag outliers\n",
    "    outlier_idx = difference > threshold\n",
    "\n",
    "    # Now it's filtered.  This should replace original values with filtered values from the rolling_median\n",
    "    # dataframe where outliers were found.\n",
    "    filtered[outlier_idx] = rolling_median[outlier_idx]\n",
    "    filtered.rename(columns={vals_col: 'FLTRD_VAL'}, inplace=True)\n",
    "\n",
    "    # Capture outliers column\n",
    "    outliers = orig_vals[outlier_idx].rename(columns={vals_col: 'OUTLIER_VAL'}).drop(columns=drop_cols)\n",
    "    # Capture outlier IS_OUTLIER column\n",
    "    outlier_idx.rename(columns={vals_col: 'IS_OUTLIER'}, inplace=True)\n",
    "\n",
    "    # The following returns a full dataframe consisting of original values columns\n",
    "    # along with the Hampel filtered data, outlier values and boolean flags where outliers found.\n",
    "    return outlier_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hampel = df.copy()\n",
    "for col in df.columns:\n",
    "    outliers = hampel_filter_df(df=df_hampel, vals_col=col, win_size=20, num_dev=3, center_win=True)\n",
    "    try:\n",
    "        df_hampel = df_hampel[~outliers.values]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=len(df_hampel.columns), figsize=(35, 6))\n",
    "for i, col in enumerate(df_hampel):\n",
    "    axs[i].plot(df_hampel[col])\n",
    "    axs[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro Savitzky-Golay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_iqr.copy()\n",
    "for col in filtered_df.columns:\n",
    "    filtered_df[col] = signal.savgol_filter(df_iqr[col], 15, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=len(filtered_df.columns), figsize=(35, 6))\n",
    "for i, col in enumerate(filtered_df):\n",
    "    axs[i].plot(filtered_df[col])\n",
    "    axs[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalonamento com MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_max = filtered_df.copy()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_min_max[df_min_max.columns] = min_max_scaler.fit_transform(filtered_df)\n",
    "df_min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalonamento com StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = filtered_df.copy()\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "df_standard[df_standard.columns] = standard_scaler.fit_transform(filtered_df)\n",
    "df_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.E. determinando slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diff_window = 1600\n",
    "var = 'Temperature'\n",
    "EE_tol_coef = 5e-5\n",
    "\n",
    "#######################################\n",
    "ee_df = df_iqr[var].copy().to_frame().reset_index(drop=True)\n",
    "ee_df[var] = signal.savgol_filter(ee_df[var], 200, 1)\n",
    "plt.plot(ee_df)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "split_list = [ee_df[i:i+n_diff_window] for i in range(0, ee_df.shape[0], n_diff_window)]\n",
    "model = LinearRegression()\n",
    "coefs = np.empty(len(split_list))\n",
    "for i, split in enumerate(split_list):\n",
    "    X = np.array(split.index)\n",
    "    y = split.values\n",
    "    model.fit(X.reshape(-1, 1), y.ravel())\n",
    "    coef_ = model.coef_ * X + model.intercept_\n",
    "    plt.plot(X, coef_)\n",
    "    coefs[i] = model.coef_[0]\n",
    "plt.show()\n",
    "plt.scatter(range(len(coefs)), coefs)\n",
    "plt.show()\n",
    "ee_true_point = np.where(coefs == 0)[0].round(5)  # se deixar livre, nunca vai ter um true EE\n",
    "if len(ee_true_point) > 0:\n",
    "    ee_index = split_list[ee_true_point[0]].index[0]\n",
    "    print(\"True EE index:\", ee_index)\n",
    "elif any(np.abs(coefs) < EE_tol_coef):\n",
    "    ee_index = split_list[np.argmin(np.abs(coefs))].index[0]\n",
    "    print(\"Aproximado EE start index:\", ee_index)\n",
    "else:\n",
    "    print(\"Nenhum EE identificado:\")\n",
    "\n",
    "if ee_index:\n",
    "    plt.plot(ee_df)\n",
    "    plt.vlines(ee_index, min(ee_df[var]), max(ee_df[var]), color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.E. com uma janela movel, sem determinar slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window_size = 1000\n",
    "rolling_speed = 500\n",
    "var = 'Temperature'\n",
    "EE_tol_coef = 5e-5\n",
    "\n",
    "#######\n",
    "ee_df = df_iqr[var].copy().to_frame().reset_index(drop=True)\n",
    "ee_df[var] = signal.savgol_filter(ee_df[var], 200, 1)\n",
    "plt.plot(ee_df)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "model = LinearRegression()\n",
    "coefs = []\n",
    "lines = []\n",
    "for i in range(0, len(ee_df), rolling_speed):\n",
    "    X = np.array(ee_df.index[i:i+rolling_window_size])\n",
    "    y = ee_df.iloc[i:i+rolling_window_size].values\n",
    "    model.fit(X.reshape(-1, 1), y.ravel())\n",
    "    coef_ = model.coef_ * X + model.intercept_\n",
    "    lines.append((X, coef_))\n",
    "    coefs.append(model.coef_[0])\n",
    "coefs = np.array(coefs)\n",
    "plt.figure()\n",
    "for line in lines:\n",
    "    plt.plot(line[0], line[1])\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(len(coefs)), coefs)\n",
    "plt.show()\n",
    "\n",
    "ee_true_point = np.where(coefs == 0)[0].round(5)  # se deixar livre, nunca vai ter um true EE\n",
    "if len(ee_true_point) > 0:\n",
    "    ee_index = lines[ee_true_point][0][0]\n",
    "    print(\"True EE index:\", ee_index)\n",
    "elif any(np.abs(coefs) < EE_tol_coef):\n",
    "    ee_index = lines[np.argmin(np.abs(coefs))][0][0]\n",
    "    print(\"Aproximado EE start index:\", ee_index)\n",
    "else:\n",
    "    print(\"Nenhum EE identificado:\")\n",
    "\n",
    "if ee_index:\n",
    "    plt.plot(ee_df)\n",
    "    plt.vlines(ee_index, min(ee_df[var]), max(ee_df[var]), color='red')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
